{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a02f6de-35b0-4db2-9228-ab93d2468af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, LeakyReLU, Dropout\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Load Dataset\n",
    "data = pd.read_csv(r\"C:\\Users\\abdul\\Desktop\\research work\\motherDataset.csv\")\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "data['RiskLevel'] = label_encoder.fit_transform(data['RiskLevel'])\n",
    "\n",
    "# Split features and target\n",
    "X = data.drop('RiskLevel', axis=1)\n",
    "y = data['RiskLevel']\n",
    "\n",
    "# Handle Imbalanced Dataset\n",
    "smoteenn = SMOTEENN(random_state=42)\n",
    "X_resampled, y_resampled = smoteenn.fit_resample(X, y)\n",
    "\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_resampled, y_resampled = adasyn.fit_resample(X_resampled, y_resampled)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.25, stratify=y_resampled, random_state=42)\n",
    "\n",
    "# Scale Features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Hyperparameter Tuning with RandomizedSearch for XGBoost\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [6, 8, 10],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "}\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "random_search_xgb = RandomizedSearchCV(xgb, xgb_param_grid, n_iter=50, cv=3, scoring='accuracy', n_jobs=-1, verbose=1, random_state=42)\n",
    "random_search_xgb.fit(X_train_scaled, y_train)\n",
    "xgb_best = random_search_xgb.best_estimator_\n",
    "\n",
    "# Hyperparameter Tuning for LightGBM\n",
    "lgbm_param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [6, 8, 10],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "}\n",
    "lgbm = LGBMClassifier(random_state=42)\n",
    "random_search_lgbm = RandomizedSearchCV(lgbm, lgbm_param_grid, n_iter=50, cv=3, scoring='accuracy', n_jobs=-1, verbose=1, random_state=42)\n",
    "random_search_lgbm.fit(X_train_scaled, y_train)\n",
    "lgbm_best = random_search_lgbm.best_estimator_\n",
    "\n",
    "# Add CatBoost Classifier\n",
    "catboost = CatBoostClassifier(verbose=0, random_state=42, learning_rate=0.005, iterations=300)\n",
    "\n",
    "# Enhanced Neural Network with RMSProp Optimizer\n",
    "def build_nn_with_rmsprop(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(256, input_dim=input_dim),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        Dropout(0.4),\n",
    "        Dense(128),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        Dropout(0.3),\n",
    "        Dense(64),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        Dropout(0.2),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.5),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "\n",
    "# Measure training time for Neural Network\n",
    "nn_model = build_nn_with_rmsprop(X_train_scaled.shape[1])\n",
    "start_time = time.time()\n",
    "nn_model.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test),\n",
    "             epochs=100, batch_size=64, verbose=1, callbacks=[early_stopping, reduce_lr])\n",
    "nn_train_time = time.time() - start_time\n",
    "\n",
    "# Measure training time for Stacking Classifier\n",
    "estimators = [('xgb', xgb_best), ('lgbm', lgbm_best), ('catboost', catboost)]\n",
    "stacking_model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), cv=5)\n",
    "start_time = time.time()\n",
    "stacking_model.fit(X_train_scaled, y_train)\n",
    "stacking_train_time = time.time() - start_time\n",
    "\n",
    "# Measure testing time for Neural Network predictions\n",
    "start_time = time.time()\n",
    "nn_probs = nn_model.predict(X_test_scaled)\n",
    "nn_test_time = time.time() - start_time\n",
    "\n",
    "# Measure testing time for Stacking Classifier predictions\n",
    "start_time = time.time()\n",
    "stacking_probs = stacking_model.predict_proba(X_test_scaled)\n",
    "stacking_test_time = time.time() - start_time\n",
    "\n",
    "\n",
    "\n",
    "# Combine predictions for hybrid model\n",
    "start_time = time.time()\n",
    "final_probs = (0.4 * stacking_probs + 0.6 * nn_probs)\n",
    "final_preds = np.argmax(final_probs, axis=1)\n",
    "hybrid_test_time = time.time() - start_time\n",
    "\n",
    "# Evaluate Hybrid Model\n",
    "hybrid_accuracy = accuracy_score(y_test, final_preds)\n",
    "hybrid_macro_precision = precision_score(y_test, final_preds, average='macro')\n",
    "hybrid_macro_recall = recall_score(y_test, final_preds, average='macro')\n",
    "hybrid_macro_f1 = f1_score(y_test, final_preds, average='macro')\n",
    "\n",
    "# Print Hybrid Model Metrics\n",
    "print(f\"\\nHybrid Model - Macro Precision: {hybrid_macro_precision:.4f}\")\n",
    "print(f\"Hybrid Model - Macro Recall: {hybrid_macro_recall:.4f}\")\n",
    "print(f\"Hybrid Model - Macro F1 Score: {hybrid_macro_f1:.4f}\")\n",
    "print(f\"Hybrid Model - Accuracy: {hybrid_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
